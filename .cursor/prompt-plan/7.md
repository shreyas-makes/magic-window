Objective: Reroute the video stream through a WebGL canvas using Pixi.js to prepare for digital zoom, and record from the canvas.

Context: Building upon the recording and storage logic. Pixi.js will be used for WebGL.

Task:
1.  **Dependencies:** Add `pixi.js` (`npm install pixi.js`).
2.  **HTML (`index.html`):**
    * Add a `<canvas id="main-canvas"></canvas>` element. Style it to be the main view area (perhaps hide the original source selection/controls initially, or place the canvas prominently). It should be 4K resolution (3840x2160), but scaled down with CSS to fit the window initially (`width: 100%; height: auto;` or similar).
    * Add an invisible `<video id="source-video" autoplay muted></video>` element.
3.  **Renderer Process (`renderer.js`):**
    * **Pixi Setup:**
        * Import `PIXI`.
        * On load, create a `PIXI.Application` attached to the `#main-canvas`, specifying the 4K resolution.
        * Get the `#source-video` element.
    * **Stream Handling Change:**
        * Modify the 'sourceSelected' logic: When a source is selected, *instead* of just sending the ID to main, the *renderer* will now get the stream.
        * Use `navigator.mediaDevices.getUserMedia({ video: { mandatory: { chromeMediaSource: 'desktop', chromeMediaSourceId: sourceId, minWidth: 3840, minHeight: 2160, maxWidth: 3840, maxHeight: 2160, minFrameRate: 60 } } })`. Handle errors.
        * If successful, set the `stream` as the `srcObject` for the invisible `#source-video` element.
        * Create a `PIXI.Texture` from the `#source-video` element: `const videoTexture = PIXI.Texture.from(videoElement);`.
        * Create a `PIXI.Sprite` using this texture and add it to the Pixi stage: `const videoSprite = new PIXI.Sprite(videoTexture); app.stage.addChild(videoSprite);`. Set sprite size to match canvas resolution.
    * **Canvas Recording Stream:**
        * Get the stream *from the canvas*: `const canvasStream = canvasElement.captureStream(60);`. Ensure this is called *after* Pixi setup.
        * Get the video track from this canvas stream: `const canvasVideoTrack = canvasStream.getVideoTracks()[0];`.
    * **IPC Change:**
        * Modify 'startRecording': *Do not* send the source ID anymore. Instead, when the user clicks "Start", ensure the `canvasVideoTrack` is ready. Send an IPC message 'startCanvasRecording' to the main process. *The track itself cannot be easily sent.* Main process will need to trust the renderer is ready.
        * Main process needs to adapt 'startRecording' to handle this trigger without needing the source ID directly. It just initiates the `MediaRecorder`.
    * **MediaRecorder Relocation (Main Process):**
        * The `MediaRecorder` setup (`new MediaRecorder`, `ondataavailable`, `onstop`, etc.) now needs to happen *in the main process*, but it needs a `MediaStream` derived from the renderer's canvas. This is the tricky part.
        * **Approach:** Use `webContents.capturePage()` periodically from the main process to capture the canvas area, encode these frames. *This is inefficient and won't meet 60FPS.*
        * **Better Approach:** Can the `canvasStream` be used directly? `MediaRecorder` is available in Renderer context. Let's move `MediaRecorder` logic *to the renderer*.
            * **Renderer (`renderer.js`):** Instantiate `MediaRecorder` here using `canvasStream`. Handle `ondataavailable`. Send the `Blob` data chunks via IPC ('sendBlobChunk') to the main process.
            * **Main Process (`main.js`):** On 'startCanvasRecording', set up the `tempSessionDir` and `segmentIndex`. Add an IPC handler for 'sendBlobChunk'. Receive the Blob data (might need conversion/handling for IPC limits, maybe base64 string or ArrayBuffer). Save the received chunk buffer to `segment_${segmentIndex}.mp4`. Increment index. Handle 'stopRecording' signal from renderer to trigger concatenation.
            * **Preload (`preload.js`):** Expose 'sendBlobChunk' sender.
4.  **Main Process (`main.js`):**
    * Adapt 'startRecording' to just set up temp dir/state, triggered by 'startCanvasRecording' from renderer.
    * Implement 'sendBlobChunk' handler to receive data and save segments.
    * Adapt 'stopRecording' logic to be triggered by an IPC message from the renderer. Concatenation logic remains in main.

Requirements:
* Use Pixi.js to render the selected video source onto a 4K canvas.
* Capture the stream from the canvas at 60 FPS.
* Move `MediaRecorder` logic to the renderer process.
* Send Blob chunks via IPC to the main process for saving as segments.
* Main process handles segment file writing and final concatenation.

Testing:
* Describe manual/TDD verification:
    * Select a source. Verify it appears correctly rendered on the canvas element.
    * Start recording. Verify segments are still being created in the temp directory in the main process.
    * Stop recording. Verify concatenation still works and the final video shows the content rendered on the canvas.
    * Check performance (FPS in renderer, CPU usage). Aim for smooth 60FPS rendering on canvas.

Output: Provide updated code for `main.js`, `preload.js`, `index.html`, and `renderer.js`. Explain the new recording flow clearly.